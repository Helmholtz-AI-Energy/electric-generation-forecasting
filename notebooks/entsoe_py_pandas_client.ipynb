{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22bc40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- How to deal with different columns in generation data for old (`DE_AT_LU` until 2018/09/30) and new bidding zone (`DE_LU` since 2018/10/01)? Old data contains all columns from new data but also additional columns, mostly about `'Actual Consumption'`, and one extra category `'Fossil Coal-derived gas Actual Aggregated'`.\n",
    "- Which time span to include in general for training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdcdbd",
   "metadata": {},
   "source": [
    "## Data-loading playground with `entsoe-py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9815c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a84800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa02137",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2013225",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6e68642c-8403-4caa-af31-bda40b8c67f6\" # web token for RESTful API\n",
    "country_code = \"10Y1001A1001A83F\" # Germany\n",
    "BZ_code = \"DE_LU\" # new bidding zone, valid since 2018/10/01\n",
    "BZ_code_old = \"DE_AT_LU\" # old bidding zone, valid until 2018/09/30\n",
    "time_zone = \"Europe/Berlin\" # time zone for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef278",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a15b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_diff_intersect(df1, df2):\n",
    "    \"\"\"\n",
    "    Return difference and intersection of columns of two dataframes.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df1 : pandas.DataFrame\n",
    "          first dataframe\n",
    "    df2 : pandas.DataFrame\n",
    "          second dataframe\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    difference in columns of df1 and df2\n",
    "    intersection of columns of df1 and df2\n",
    "    \n",
    "    \"\"\"\n",
    "    return df1.columns.difference(df2.columns), df1.columns.intersection(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12004015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_intervals(start_date, \n",
    "                       end_date, \n",
    "                       time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Get time points for sequential data loading from ENTSO-E transparency platform.\n",
    "    \n",
    "    For one request, the time delta for loading data from the platform is limited to one year.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    pandas series with timestamps of time points to consider between start and end date\n",
    "    \"\"\"\n",
    "    # Convert start and end dates to timestamps.\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "\n",
    "    # Create series from start and end timestamps.\n",
    "    start_series = pd.Series(pd.Timestamp(start_date))\n",
    "    end_series = pd.Series(pd.Timestamp(end_date))\n",
    "    \n",
    "    # Create date range from start and end dates and determine year starts within range.\n",
    "    # Convert data range to series.\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"YS\", inclusive=\"both\").to_series()\n",
    "\n",
    "    # Check whether start date itself is year start.\n",
    "    # If not, prepend to dates to consider for data loading.\n",
    "    if not start.is_year_start:\n",
    "        dates = pd.concat([start_series, dates], ignore_index=True)\n",
    "\n",
    "    # Check whether end date itself is year start.\n",
    "    # If not, append to dates to consider for data loading.\n",
    "    if not end.is_year_start:\n",
    "        dates = pd.concat([dates, end_series], ignore_index=True)\n",
    "        \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaafbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(start_date, \n",
    "              end_date, \n",
    "              api_key, \n",
    "              country_code=\"10Y1001A1001A83F\", \n",
    "              time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Load actual load and actual aggregated generation per production type for requested time interval.\n",
    "    \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "                \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with time points as indices and load + generation per type as columns.\n",
    "    \"\"\"\n",
    "    from entsoe import EntsoePandasClient\n",
    "    # Initialize client and settings.\n",
    "    client = EntsoePandasClient(api_key=api_key)\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "    # Query data and save to dataframe.\n",
    "    df_load = client.query_load(country_code, start=start, end=end)\n",
    "    print(f\"Actual load has shape {df_load.shape}.\")\n",
    "    df_gen = client.query_generation(country_code, start=start, end=end, psr_type=None)\n",
    "    df_gen.columns = [\" \".join(a) for a in df_gen.columns.to_flat_index()]\n",
    "    print(f\"Actual generation per production type has shape {df_gen.shape}.\")\n",
    "    df_final = pd.concat([df_load, df_gen], axis=1) # Concatenate dataframes in columns dimension.\n",
    "    print(f\"Concatenated data frame has shape {df_final.shape}.\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822010db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start_date, \n",
    "               end_date, \n",
    "               api_key, \n",
    "               country_code=\"10Y1001A1001A83F\", \n",
    "               time_zone=\"Europe/Berlin\",\n",
    "               drop_consumption=True,\n",
    "               create_PSLP_columns=True):\n",
    "    \"\"\"\n",
    "    Fetch data from ENTSO-E transparency platform as requested.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    drop_consumption : Bool\n",
    "                       Drop columns containing actual consumption.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with actual load and generation per type for requested time interval\n",
    "    \"\"\"\n",
    "    # Determine sequence of dates to consider when loading data.\n",
    "    dates = get_load_intervals(start_date, end_date, time_zone)\n",
    "    print(f\"Consider the following dates:\\n{dates}\")\n",
    "    df_list = []\n",
    "    \n",
    "    for i, _ in enumerate(dates):\n",
    "\n",
    "        if i == dates.shape[0] - 1:\n",
    "            print(\"Returning final data frame...\")\n",
    "            df_final = pd.concat(df_list, axis=0) # Concatenate dataframes along time axis (index).\n",
    "            # Drop columns containing actual consumption?\n",
    "            if drop_consumption:\n",
    "                print(\"Dropping columns containing actual consumption...\")\n",
    "                df_final.drop(list(df_final.filter(regex='Consumption')), axis=1, inplace=True)\n",
    "            original_headers = df_final.columns\n",
    "            # Create PSLP columns?\n",
    "            if create_PSLP_columns:\n",
    "                print(\"Creating columns for PSLP calculation...\")\n",
    "                for header in original_headers:\n",
    "                    df_final[str(header) + \" PSLP\"] = pd.Series(dtype='float')\n",
    "            return df_final, original_headers\n",
    "            \n",
    "        try:\n",
    "            print(f\"Trying to load data chunk for time interval [{dates[i]}, {dates[i+1]}]...\")\n",
    "            df_temp = load_data(start_date=dates[i], \n",
    "                                end_date=dates[i+1],\n",
    "                                api_key=api_key,\n",
    "                                time_zone=time_zone,\n",
    "                                country_code=country_code)\n",
    "            print(df_temp.shape)\n",
    "            df_list.append(df_temp)\n",
    "            print(\"Loading successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Loading failed!\", e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3415492a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following dates:\n",
      "0   2017-12-25\n",
      "1   2018-01-01\n",
      "2   2018-12-25\n",
      "dtype: datetime64[ns]\n",
      "Trying to load data chunk for time interval [2017-12-25 00:00:00, 2018-01-01 00:00:00]...\n",
      "Actual load has shape (672, 1).\n",
      "Actual generation per production type has shape (672, 32).\n",
      "Concatenated data frame has shape (672, 33).\n",
      "(672, 33)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2018-01-01 00:00:00, 2018-12-25 00:00:00]...\n",
      "Actual load has shape (34368, 1).\n",
      "Actual generation per production type has shape (34368, 25).\n",
      "Concatenated data frame has shape (34368, 26).\n",
      "(34368, 26)\n",
      "Loading successful!\n",
      "Returning final data frame...\n",
      "Dropping columns containing actual consumption...\n",
      "Creating columns for PSLP calculation...\n"
     ]
    }
   ],
   "source": [
    "start_date = \"20171225\"\n",
    "end_date = \"20181225\"\n",
    "df_test, original_headers = fetch_data(start_date, end_date, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a45721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _correct_time_shift(df, usual_length=96):\n",
    "    unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "    unique_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in unique_dates]\n",
    "    time_shift_dates = []\n",
    "    for date in unique_dates:\n",
    "        length = df.loc[date].shape[0]\n",
    "        if length != usual_length:\n",
    "            print(f\"Time shift at {date}, length is {length}.\")\n",
    "            time_shift_dates.append([date, length])\n",
    "    return time_shift_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86820446",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35040, 36)\n",
      "Index(['Actual Load', 'Biomass Actual Aggregated',\n",
      "       'Fossil Brown coal/Lignite Actual Aggregated',\n",
      "       'Fossil Coal-derived gas Actual Aggregated',\n",
      "       'Fossil Gas Actual Aggregated', 'Fossil Hard coal Actual Aggregated',\n",
      "       'Fossil Oil Actual Aggregated', 'Geothermal Actual Aggregated',\n",
      "       'Hydro Pumped Storage Actual Aggregated',\n",
      "       'Hydro Run-of-river and poundage Actual Aggregated',\n",
      "       'Hydro Water Reservoir Actual Aggregated', 'Nuclear Actual Aggregated',\n",
      "       'Other Actual Aggregated', 'Other renewable Actual Aggregated',\n",
      "       'Solar Actual Aggregated', 'Waste Actual Aggregated',\n",
      "       'Wind Offshore Actual Aggregated', 'Wind Onshore Actual Aggregated',\n",
      "       'Actual Load PSLP', 'Biomass Actual Aggregated PSLP',\n",
      "       'Fossil Brown coal/Lignite Actual Aggregated PSLP',\n",
      "       'Fossil Coal-derived gas Actual Aggregated PSLP',\n",
      "       'Fossil Gas Actual Aggregated PSLP',\n",
      "       'Fossil Hard coal Actual Aggregated PSLP',\n",
      "       'Fossil Oil Actual Aggregated PSLP',\n",
      "       'Geothermal Actual Aggregated PSLP',\n",
      "       'Hydro Pumped Storage Actual Aggregated PSLP',\n",
      "       'Hydro Run-of-river and poundage Actual Aggregated PSLP',\n",
      "       'Hydro Water Reservoir Actual Aggregated PSLP',\n",
      "       'Nuclear Actual Aggregated PSLP', 'Other Actual Aggregated PSLP',\n",
      "       'Other renewable Actual Aggregated PSLP',\n",
      "       'Solar Actual Aggregated PSLP', 'Waste Actual Aggregated PSLP',\n",
      "       'Wind Offshore Actual Aggregated PSLP',\n",
      "       'Wind Onshore Actual Aggregated PSLP'],\n",
      "      dtype='object')\n",
      "0\n",
      "Index(['Actual Load', 'Biomass Actual Aggregated',\n",
      "       'Fossil Brown coal/Lignite Actual Aggregated',\n",
      "       'Fossil Coal-derived gas Actual Aggregated',\n",
      "       'Fossil Gas Actual Aggregated', 'Fossil Hard coal Actual Aggregated',\n",
      "       'Fossil Oil Actual Aggregated', 'Geothermal Actual Aggregated',\n",
      "       'Hydro Pumped Storage Actual Aggregated',\n",
      "       'Hydro Run-of-river and poundage Actual Aggregated',\n",
      "       'Hydro Water Reservoir Actual Aggregated', 'Nuclear Actual Aggregated',\n",
      "       'Other Actual Aggregated', 'Other renewable Actual Aggregated',\n",
      "       'Solar Actual Aggregated', 'Waste Actual Aggregated',\n",
      "       'Wind Offshore Actual Aggregated', 'Wind Onshore Actual Aggregated'],\n",
      "      dtype='object')\n",
      "Time shift at 2018-03-25, length is 92.\n",
      "Time shift at 2018-10-28, length is 100.\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test.columns)\n",
    "print(df_test[\"Actual Load\"].isna().sum())\n",
    "print(original_headers)\n",
    "time_shifted = _correct_time_shift(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e63633cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Load                                               43926.0\n",
      "Biomass Actual Aggregated                                  4731.0\n",
      "Fossil Brown coal/Lignite Actual Aggregated                5230.0\n",
      "Fossil Coal-derived gas Actual Aggregated                   433.0\n",
      "Fossil Gas Actual Aggregated                               1021.0\n",
      "Fossil Hard coal Actual Aggregated                         1443.0\n",
      "Fossil Oil Actual Aggregated                                174.0\n",
      "Geothermal Actual Aggregated                                 26.0\n",
      "Hydro Pumped Storage Actual Aggregated                      418.0\n",
      "Hydro Run-of-river and poundage Actual Aggregated          1507.0\n",
      "Hydro Water Reservoir Actual Aggregated                      58.0\n",
      "Nuclear Actual Aggregated                                  6220.0\n",
      "Other Actual Aggregated                                    2845.0\n",
      "Other renewable Actual Aggregated                           149.0\n",
      "Solar Actual Aggregated                                       0.0\n",
      "Waste Actual Aggregated                                     628.0\n",
      "Wind Offshore Actual Aggregated                            3041.0\n",
      "Wind Onshore Actual Aggregated                            27582.0\n",
      "Actual Load PSLP                                              NaN\n",
      "Biomass Actual Aggregated PSLP                                NaN\n",
      "Fossil Brown coal/Lignite Actual Aggregated PSLP              NaN\n",
      "Fossil Coal-derived gas Actual Aggregated PSLP                NaN\n",
      "Fossil Gas Actual Aggregated PSLP                             NaN\n",
      "Fossil Hard coal Actual Aggregated PSLP                       NaN\n",
      "Fossil Oil Actual Aggregated PSLP                             NaN\n",
      "Geothermal Actual Aggregated PSLP                             NaN\n",
      "Hydro Pumped Storage Actual Aggregated PSLP                   NaN\n",
      "Hydro Run-of-river and poundage Actual Aggregated PSLP        NaN\n",
      "Hydro Water Reservoir Actual Aggregated PSLP                  NaN\n",
      "Nuclear Actual Aggregated PSLP                                NaN\n",
      "Other Actual Aggregated PSLP                                  NaN\n",
      "Other renewable Actual Aggregated PSLP                        NaN\n",
      "Solar Actual Aggregated PSLP                                  NaN\n",
      "Waste Actual Aggregated PSLP                                  NaN\n",
      "Wind Offshore Actual Aggregated PSLP                          NaN\n",
      "Wind Onshore Actual Aggregated PSLP                           NaN\n",
      "Name: 2017-12-25 00:00:00+01:00, dtype: float64\n",
      "2017-12-25 00:00:00+01:00\n",
      "2018-03-25 00:00:00+01:00 UTC+01:00 1:00:00\n",
      "2018-03-25 02:00:00+01:00 UTC+01:00 1:00:00\n"
     ]
    }
   ],
   "source": [
    "df_test.index = pd.to_datetime(df_test.index, utc=True).tz_convert(tz=\"UTC+01:00\")\n",
    "print(df_test.iloc[0])\n",
    "print(df_test.index[0])\n",
    "print(df_test.loc[\"2018-03-25\"].index[0], df_test.loc[\"2018-03-25\"].index[0].tz, \n",
    "      df_test.loc[\"2018-03-25\"].index[0].utcoffset())\n",
    "print(df_test.loc[\"2018-03-25\"].index[8], df_test.loc[\"2018-03-25\"].index[8].tz, \n",
    "      df_test.loc[\"2018-03-25\"].index[8].utcoffset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d919384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "time_shifted_corr = _correct_time_shift(df_test)\n",
    "print(time_shifted_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pslp_category(date, weekday=None, holiday=None, country_code='DE'):\n",
    "    \"\"\"\n",
    "    Get PSLP category from date, weekday information, and holiday information.\n",
    "    0 : weekday\n",
    "    1 : Saturday\n",
    "    2 : Sunday and holiday\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    date : str\n",
    "           date in 'YYYYMMDD' format\n",
    "    weekday : int\n",
    "              corresponding weekday\n",
    "              0 - Mon, 1 - Tue, 2 - Wed, 3 - Thu, 4 - Fri, 5 - Sat, 6 - Sun\n",
    "    holiday : Bool\n",
    "              True if public holiday, False if not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int : PSLP category\n",
    "    \"\"\"\n",
    "    # Convert string-type date to datetime object.\n",
    "    if type(date) is str:\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    # Assign weekday if not given.\n",
    "    if weekday is None:\n",
    "        weekday = date.weekday()\n",
    "    \n",
    "    # Assign holiday category if not given.\n",
    "    if holiday is None:\n",
    "        import holidays\n",
    "        holiday = date in holidays.country_holidays(country_code)\n",
    "    \n",
    "    # Special treatment for Christmas eve and New year's eve as Saturdays.\n",
    "    if ( date.day == 24 or date.day == 31 ) and date.month == 12 and weekday != 6:\n",
    "        pslp_category = 1\n",
    "    # weekdays\n",
    "    elif weekday < 5 and holiday is False:\n",
    "        pslp_category = 0\n",
    "    # Saturdays\n",
    "    elif weekday == 5 and holiday is False:\n",
    "        pslp_category = 1\n",
    "    # Sundays and holidays\n",
    "    elif weekday == 6 or holiday is True:\n",
    "        pslp_category = 2\n",
    "    return pslp_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_pslp_categories(df, country_code='DE'):\n",
    "    \"\"\"\n",
    "    Assign PSLP categories to dates in dataframe's datetime index.\n",
    "\n",
    "    0 is weekday, 1 is Saturday, 2 is Sunday or holiday.\n",
    "    Special treatment for Christmas eve and New Year's eve (as Saturdays).\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "    country_code : str\n",
    "                   country to determine holidays for\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Dataframe\n",
    "    Dataframe amended by weekday information, holiday information, and PSLP category\n",
    "    \"\"\"\n",
    "\n",
    "    import holidays\n",
    "    \n",
    "    # Get holidays in specified country.\n",
    "    country_holidays = holidays.country_holidays(country_code) # Passing a state is also possible!\n",
    "\n",
    "    s = df.index.to_series()                                # Convert datetime index to series.\n",
    "    dates = s.dt.date                                       # Get plain dates from datetime objects.\n",
    "    weekdays = s.dt.weekday                                 # Get weekdays from datetime objects.\n",
    "    holidays = [date in country_holidays for date in dates] # Determine holidays.\n",
    "    pslp_category = []\n",
    "    \n",
    "    for d, wd, hd in zip(dates, weekdays, holidays):\n",
    "        pslp_category.append(get_pslp_category(d, wd, hd))\n",
    "        \n",
    "    df[\"PSLP Category\"] = pslp_category\n",
    "    df[\"Holiday\"] = holidays\n",
    "    df[\"Weekday\"] = weekdays\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ad184",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_str = \"20181028\"\n",
    "df_test = assign_pslp_categories(df_test)\n",
    "print(df_test.loc[date_str])\n",
    "print(df_test.columns)\n",
    "print(df_test[\"PSLP Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_pslp(df, original_headers, date_str, lookback=3, country_code='DE', DEBUG=False):\n",
    "    \"\"\"\n",
    "    Calculate PSLPs for all dates in dataframe or for given date from given data.\n",
    "    \n",
    "    The data is categorized into weekdays, Saturdays, and Sundays/holidays.\n",
    "    The `lookback` most recent days from the specified date's category are used to\n",
    "    calculate the corresponding PSLP as the average.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "         data to calculate PSLP for, must have datetime index\n",
    "    original_headers : list of str\n",
    "                       categories to calculate PSLP for\n",
    "    date_str : str\n",
    "               date 'YYYYMMDD' to calculate PSLP for; if None, calculate PSLP for all dates\n",
    "    lookback : int\n",
    "               number of days to consider in each category for calculating PSLP\n",
    "    country_code : str\n",
    "                   considered country (for holidays)\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "    df = assign_pslp_categories(df, country_code)\n",
    "\n",
    "    print(f\"Calculating PSLP for date {date_str}...\")\n",
    "    date = pd.to_datetime(date_str)\n",
    "\n",
    "    # Check whether date is in range of given dataframe.\n",
    "    if date.date() < unique_dates[0]:\n",
    "        raise ValueError(f\"PSLP cannot be calculated. Date {date_str} is in the past of data.\")\n",
    "    if date.date()  > unique_dates[-1] + pd.Timedelta(days = 1):\n",
    "        raise ValueError(f\"PSLP cannot be calculated. Date {date_str} is more than 1d in the future of data.\")\n",
    "    assert date.date() in unique_dates\n",
    "\n",
    "    pslp_category = get_pslp_category(date_str)\n",
    "    print(f\"PSLP category of {date.date()} is {pslp_category}.\")\n",
    "    df_pslp = df[df['PSLP Category'] == pslp_category]\n",
    "    \n",
    "    unique_dates_pslp = df_pslp.index.to_series().dt.date.drop_duplicates().tolist()\n",
    "    idx_pslp = unique_dates_pslp.index(date.date())\n",
    "    print(f\"Index in unique days of PSLP category is {idx_pslp}.\")\n",
    "    if idx_pslp - lookback < 0:\n",
    "        raise ValueError(f\"PSLP cannot be calculated. Less than {lookback} samples in PSLP category for date {date_str}.\")\n",
    "    lookback_dates = unique_dates_pslp[idx_pslp-lookback:idx_pslp]\n",
    "    lookback_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in lookback_dates]\n",
    "    print(f\"Dates to consider for calculating PSLP: {lookback_dates}\")\n",
    "    for header in original_headers:\n",
    "        print(f\"Looking at {header}...\")\n",
    "        for idx, d in enumerate(lookback_dates):\n",
    "            if idx == 0:\n",
    "                df[header + \" PSLP\"].at[date_str] = df[header].loc[d]\n",
    "                if DEBUG:\n",
    "                    print(df[header + \" PSLP\"].at[date_str])\n",
    "            else:\n",
    "                temp1 = df[header + \" PSLP\"].at[date_str].reset_index(drop=True)\n",
    "                temp2 = df[header].at[d].reset_index(drop=True)\n",
    "                temp = temp1 + temp2\n",
    "                df[header + \" PSLP\"].at[date_str] = temp\n",
    "                if DEBUG:\n",
    "                    print(df[header + \" PSLP\"].at[date_str])\n",
    "        df[header + \" PSLP\"].at[date_str] /= lookback\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a752669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_test[\"Actual Load PSLP\"][\"20181028\"])\n",
    "print(df_test[\"Actual Load\"][\"2018-10-28 01:00:00+02:00\"])\n",
    "#print(df_test[\"Actual Load\"][\"20181007\"])\n",
    "pd.set_option('display.max_rows', 100)\n",
    "print(df_test[\"Actual Load\"][\"2018-10-28\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d8f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_debug = _calculate_pslp(df_test, original_headers, \"20181028\", lookback=3, country_code='DE', DEBUG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f09a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pslps(df, original_headers, date_str=None, lookback=3, country_code='DE', DEBUG=False):\n",
    "    \"\"\"\n",
    "    Calculate PSLPs for all dates in dataframe or for given date from given data.\n",
    "    \n",
    "    The data is categorized into weekdays, Saturdays, and Sundays/holidays.\n",
    "    The `lookback` most recent days from the specified date's category are used to\n",
    "    calculate the corresponding PSLP as the average.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "         data to calculate PSLP for, must have datetime index\n",
    "    original_headers : list of str\n",
    "                       categories to calculate PSLP for\n",
    "    date_str : str\n",
    "               date 'YYYYMMDD' to calculate PSLP for; if None, calculate PSLP for all dates\n",
    "    lookback : int\n",
    "               number of days to consider in each category for calculating PSLP\n",
    "    country_code : str\n",
    "                   considered country (for holidays)\n",
    "    \n",
    "    \"\"\"\n",
    "    if date_str is not None:\n",
    "        print(f\"Calculating PSLP for date {date_str} only...\")\n",
    "        df = _calculate_pslp(df, \n",
    "                            original_headers, \n",
    "                            date_str, \n",
    "                            lookback=lookback, \n",
    "                            country_code=country_code, \n",
    "                            DEBUG=DEBUG)\n",
    "    \n",
    "    else:\n",
    "        print(\"Calculating PSLPs for all dates in dataframe...\")\n",
    "        unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "        unique_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in unique_dates]\n",
    "        \n",
    "        for date in unique_dates:\n",
    "            try:\n",
    "                df = _calculate_pslp(df, \n",
    "                                    original_headers, \n",
    "                                    date_str=date, \n",
    "                                    lookback=lookback, \n",
    "                                    country_code=country_code, \n",
    "                                    DEBUG=DEBUG)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_str = \"20181208\"\n",
    "df_Pslps= calculate_pslps(df_test, original_headers)#, date_str)\n",
    "#print(df_Pslps[\"Actual Load PSLP\"].loc[date_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f96f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Pslps[\"20181028\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f19cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(df_Pslps)\n",
    "df_Pslps[\"Actual Load Residuals\"] = df_Pslps[\"Actual Load\"] - df_Pslps[\"Actual Load PSLP\"]\n",
    "print(df_Pslps[\"Actual Load Residuals\"].loc[date_str])\n",
    "print(df_Pslps[\"Actual Load\"].loc[date_str])\n",
    "print(df_Pslps[\"Actual Load PSLP\"].loc[date_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Plot PSLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dabc64",
   "metadata": {},
   "source": [
    "### Complete parameter list\n",
    "https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_complete_parameter_list\n",
    "\n",
    "### Queries returning Pandas Series\n",
    "\n",
    "`client.query_day_ahead_prices(country_code, start=start,end=end)` <br>\n",
    "`client.query_net_position(country_code, start=start, end=end, dayahead=True)` <br>\n",
    "`client.query_crossborder_flows(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_scheduled_exchanges(country_code_from, country_code_to, start, end, dayahead=False)` <br>\n",
    "`client.query_net_transfer_capacity_dayahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_weekahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_monthahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_yearahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_intraday_offered_capacity(country_code_from, country_code_to, start, end,implicit=True)` <br>\n",
    "`client.query_offered_capacity(country_code_from, country_code_to, start, end, contract_marketagreement_type, implicit=True)` <br>\n",
    "`client.query_aggregate_water_reservoirs_and_hydro_storage(country_code, start, end)`\n",
    "\n",
    "### Queries returning Pandas DataFrames\n",
    "\n",
    "`client.query_load(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_and_forecast(country_code, start=start, end=end)` <br>\n",
    "`client.query_generation_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_wind_and_solar_forecast(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation_per_plant(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity_per_unit(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_imbalance_prices(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_prices(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_amount(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_unavailability_of_generation_units(country_code, start=start,end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_transmission(country_code_from, country_code_to, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_withdrawn_unavailability_of_generation_units(country_code, start, end)` <br>\n",
    "`client.query_import(country_code, start, end)` <br>\n",
    "`client.query_generation_import(country_code, start, end)` <br>\n",
    "`client.query_procured_balancing_capacity(country_code, start, end, process_type, type_marketagreement_type=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c395d5",
   "metadata": {},
   "source": [
    "## Load data from client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"load forecast\"] = client.query_load_forecast(country_code, start=start,end=end)\n",
    "df[\"load\"] = client.query_load(country_code, start=start,end=end)\n",
    "df[\"load forecast error\"] = df[\"load forecast\"] - df[\"load\"]\n",
    "df[\"generation forecast\"] = client.query_generation_forecast(country_code, start=start,end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = client.query_generation(country_code, start=start,end=end, psr_type=None)\n",
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64521a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0978cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "ax.plot(df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation\"] = df_gen.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation forecast error\"] = df[\"generation forecast\"] - df[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b9c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87d813",
   "metadata": {},
   "source": [
    "## Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82506c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('entsoe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
