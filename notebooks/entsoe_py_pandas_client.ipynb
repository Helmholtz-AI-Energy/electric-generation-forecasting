{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22bc40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- How to deal with different columns in generation data for old (`DE_AT_LU` until 2018/09/30) and new bidding zone (`DE_LU` since 2018/10/01)? Old data contains all columns from new data but also additional columns, mostly about `'Actual Consumption'`, and one extra category `'Fossil Coal-derived gas Actual Aggregated'`.\n",
    "- Which time span to include in general for training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdcdbd",
   "metadata": {},
   "source": [
    "## Data-loading playground with `entsoe-py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9815c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa02137",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2013225",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6e68642c-8403-4caa-af31-bda40b8c67f6\" # web token for RESTful API\n",
    "country_code = \"10Y1001A1001A83F\" # Germany\n",
    "BZ_code = \"DE_LU\" # new bidding zone, valid since 2018/10/01\n",
    "BZ_code_old = \"DE_AT_LU\" # old bidding zone, valid until 2018/09/30\n",
    "time_zone = \"Europe/Berlin\" # time zone for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef278",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a15b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_diff_intersect(df1, df2):\n",
    "    \"\"\"\n",
    "    Return difference and intersection of columns of two dataframes.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df1 : pandas.DataFrame\n",
    "          first dataframe\n",
    "    df2 : pandas.DataFrame\n",
    "          second dataframe\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    difference in columns of df1 and df2\n",
    "    intersection of columns of df1 and df2\n",
    "    \n",
    "    \"\"\"\n",
    "    return df1.columns.difference(df2.columns), df1.columns.intersection(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12004015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_intervals(start_date, end_date, time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Get time points for sequential data loading from ENTSO-E transparency platform.\n",
    "    \n",
    "    For one request, the time delta for loading data from the platform is limited to one year.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    pandas series with timestamps of time points to consider between start and end date\n",
    "    \"\"\"\n",
    "    # Convert start and end dates to timestamps.\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "\n",
    "    # Create series from start and end timestamps.\n",
    "    start_series = pd.Series(pd.Timestamp(start_date))\n",
    "    end_series = pd.Series(pd.Timestamp(end_date))\n",
    "    \n",
    "    # Create date range from start and end dates and determine year starts within range.\n",
    "    # Convert data range to series.\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"YS\", inclusive=\"both\").to_series()\n",
    "\n",
    "    # Check whether start date itself is year start.\n",
    "    # If not, prepend to dates to consider for data loading.\n",
    "    if not start.is_year_start:\n",
    "        dates = pd.concat([start_series, dates], ignore_index=True)\n",
    "\n",
    "    # Check whether end date itself is year start.\n",
    "    # If not, append to dates to consider for data loading.\n",
    "    if not end.is_year_start:\n",
    "        dates = pd.concat([dates, end_series], ignore_index=True)\n",
    "        \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(start_date, \n",
    "              end_date, \n",
    "              api_key, \n",
    "              country_code=\"10Y1001A1001A83F\", \n",
    "              time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Load actual load and actual aggregated generation per production type for requested time interval.\n",
    "    \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "                \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with time points as indices and load + generation per type as columns.\n",
    "    \"\"\"\n",
    "    from entsoe import EntsoePandasClient\n",
    "    # Initialize client and settings.\n",
    "    client = EntsoePandasClient(api_key=api_key)\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "    # Query data and save to dataframe.\n",
    "    df_load = client.query_load(country_code, start=start, end=end)\n",
    "    print(f\"Actual load has shape {df_load.shape}.\")\n",
    "    df_gen = client.query_generation(country_code, start=start, end=end, psr_type=None)\n",
    "    df_gen.columns = [\" \".join(a) for a in df_gen.columns.to_flat_index()]\n",
    "    print(f\"Actual generation per production type has shape {df_gen.shape}.\")\n",
    "    df_final = pd.concat([df_load, df_gen], axis=1) # Concatenate dataframes in columns dimension.\n",
    "    print(f\"Concatenated data frame has shape {df_final.shape}.\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822010db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start_date, \n",
    "               end_date, \n",
    "               api_key, \n",
    "               country_code=\"10Y1001A1001A83F\", \n",
    "               time_zone=\"Europe/Berlin\",\n",
    "               drop_consumption=True,\n",
    "               create_pslp_columns=True):\n",
    "    \"\"\"\n",
    "    Fetch data from ENTSO-E transparency platform as requested.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    drop_consumption : Bool\n",
    "                       Drop columns containing actual consumption.\n",
    "    create_pslp_columns : Bool\n",
    "                          Create columns for subsequent PSLP and residuals calculation.    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with actual load and generation per type for requested time interval\n",
    "    \"\"\"\n",
    "    # Determine sequence of dates to consider when loading data.\n",
    "    dates = get_load_intervals(start_date, end_date, time_zone)\n",
    "    print(f\"Consider the following dates:\\n{dates}\")\n",
    "    df_list = []\n",
    "    \n",
    "    for i, _ in enumerate(dates):\n",
    "\n",
    "        if i == dates.shape[0] - 1:\n",
    "            print(\"Returning final data frame...\")\n",
    "            df_final = pd.concat(df_list, axis=0) # Concatenate dataframes along time axis (index).\n",
    "            df_final.index = pd.to_datetime(df_final.index, utc=True).tz_convert(tz=\"UTC+01:00\")\n",
    "\n",
    "            # Drop columns containing actual consumption?\n",
    "            if drop_consumption:\n",
    "                print(\"Dropping columns containing actual consumption...\")\n",
    "                df_final.drop(list(df_final.filter(regex='Consumption')), axis=1, inplace=True)\n",
    "            original_headers = df_final.columns\n",
    "            # Create PSLP columns?\n",
    "            if create_pslp_columns:\n",
    "                print(\"Creating columns for PSLP calculation...\")\n",
    "                for header in original_headers:\n",
    "                    df_final[str(header) + \" PSLP\"] = pd.Series(dtype='float')\n",
    "            return df_final, original_headers\n",
    "            \n",
    "        try:\n",
    "            print(f\"Trying to load data chunk for time interval [{dates[i]}, {dates[i+1]}]...\")\n",
    "            df_temp = load_data(start_date=dates[i], \n",
    "                                end_date=dates[i+1],\n",
    "                                api_key=api_key,\n",
    "                                time_zone=time_zone,\n",
    "                                country_code=country_code)\n",
    "            print(df_temp.shape)\n",
    "            df_list.append(df_temp)\n",
    "            print(\"Loading successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Loading failed!\", e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415492a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = \"20171225\"\n",
    "end_date = \"20181225\"\n",
    "df_test, original_headers = fetch_data(start_date, end_date, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _correct_time_shift(df, usual_length=96):\n",
    "    \"\"\"\n",
    "    Find CET-CEST time shift dates in dataframe index.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "         considered dataframe\n",
    "    usual_length : int\n",
    "                   usual length of one day (96 for 15 min frequency)\n",
    "                   \n",
    "    Returns\n",
    "    -------\n",
    "    list : list of lists with time-shifting dates and their respective lengths\n",
    "    \"\"\"\n",
    "    unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "    unique_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in unique_dates]\n",
    "    time_shift_dates = []\n",
    "    for date in unique_dates:\n",
    "        length = df.loc[date].shape[0]\n",
    "        if length != usual_length:\n",
    "            print(f\"Time shift at {date}, length is {length}.\")\n",
    "            time_shift_dates.append([date, length])\n",
    "    return time_shift_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86820446",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test.columns)\n",
    "print(df_test[\"Actual Load\"].isna().sum())\n",
    "print(original_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pslp_category(date, weekday=None, holiday=None, country_code='DE'):\n",
    "    \"\"\"\n",
    "    Get PSLP category from date, weekday information, and holiday information.\n",
    "    0 : weekday\n",
    "    1 : Saturday\n",
    "    2 : Sunday and holiday\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    date : str\n",
    "           date in 'YYYYMMDD' format\n",
    "    weekday : int\n",
    "              corresponding weekday\n",
    "              0 - Mon, 1 - Tue, 2 - Wed, 3 - Thu, 4 - Fri, 5 - Sat, 6 - Sun\n",
    "    holiday : Bool\n",
    "              True if public holiday, False if not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int : PSLP category\n",
    "    \"\"\"\n",
    "    # Convert string-type date to datetime object.\n",
    "    if type(date) is str:\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    # Assign weekday if not given.\n",
    "    if weekday is None:\n",
    "        weekday = date.weekday()\n",
    "    \n",
    "    # Assign holiday category if not given.\n",
    "    if holiday is None:\n",
    "        import holidays\n",
    "        holiday = date in holidays.country_holidays(country_code)\n",
    "    \n",
    "    # Special treatment for Christmas eve and New year's eve as Saturdays.\n",
    "    if ( date.day == 24 or date.day == 31 ) and date.month == 12 and weekday != 6:\n",
    "        pslp_category = 1\n",
    "    # weekdays\n",
    "    elif weekday < 5 and holiday is False:\n",
    "        pslp_category = 0\n",
    "    # Saturdays\n",
    "    elif weekday == 5 and holiday is False:\n",
    "        pslp_category = 1\n",
    "    # Sundays and holidays\n",
    "    elif weekday == 6 or holiday is True:\n",
    "        pslp_category = 2\n",
    "    return pslp_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b26025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_pslp_categories(df, country_code='DE'):\n",
    "    \"\"\"\n",
    "    Assign PSLP categories to dates in dataframe's datetime index.\n",
    "\n",
    "    0 is weekday, 1 is Saturday, 2 is Sunday or holiday.\n",
    "    Special treatment for Christmas eve and New Year's eve (as Saturdays).\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "    country_code : str\n",
    "                   country to determine holidays for\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Dataframe\n",
    "    Dataframe amended by weekday information, holiday information, and PSLP category\n",
    "    \"\"\"\n",
    "    import holidays\n",
    "    \n",
    "    # Get holidays in specified country.\n",
    "    country_holidays = holidays.country_holidays(country_code) # Passing a state is also possible!\n",
    "\n",
    "    s = df.index.to_series()                                # Convert datetime index to series.\n",
    "    dates = s.dt.date                                       # Get plain dates from datetime objects.\n",
    "    weekdays = s.dt.weekday                                 # Get weekdays from datetime objects.\n",
    "    holidays = [date in country_holidays for date in dates] # Determine holidays.\n",
    "    pslp_category = []\n",
    "    \n",
    "    for d, wd, hd in zip(dates, weekdays, holidays):\n",
    "        pslp_category.append(get_pslp_category(d, wd, hd))\n",
    "        \n",
    "    df[\"PSLP Category\"] = pslp_category\n",
    "    df[\"Holiday\"] = holidays\n",
    "    df[\"Weekday\"] = weekdays\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c574b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_str = \"20181028\"\n",
    "df_test = assign_pslp_categories(df_test)\n",
    "print(df_test.loc[date_str])\n",
    "print(df_test.columns)\n",
    "print(df_test[\"PSLP Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nearest_future_pslp_date(date_str, pslp_category=None):\n",
    "    \"\"\"\n",
    "    For a given date, get nearest days in future for each PSLP category.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    date_str : str\n",
    "               considered date\n",
    "    pslp_category : int\n",
    "                    PSLP category of considered date\n",
    "    \"\"\"\n",
    "    if pslp_category is None:\n",
    "        pslp_category = get_pslp_category(date_str)\n",
    "    start = pd.to_datetime(date_str).date() + pd.Timedelta(days=1)\n",
    "    end = pd.to_datetime(date_str).date() + pd.Timedelta(weeks=1)\n",
    "    future_dates = pd.date_range(start=start.strftime('%Y%m%d'), end=end.strftime('%Y%m%d'))\n",
    "    pslp_categories = np.array([get_pslp_category(d) for d in future_dates])\n",
    "    idx = np.where(pslp_categories == pslp_category)\n",
    "    return future_dates[idx][0].date()\n",
    "    \n",
    "fut_date = _get_nearest_future_pslp_date('20180909')\n",
    "print(pd.to_datetime(\"20180916\").date() != fut_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_pslp(df, original_headers, date_str, lookback=3, country_code='DE', DEBUG=False):\n",
    "    \"\"\"\n",
    "    Calculate PSLPs for all dates in dataframe or for given date from given data.\n",
    "    \n",
    "    The data is categorized into weekdays, Saturdays, and Sundays/holidays.\n",
    "    The `lookback` most recent days from the specified date's category are used to\n",
    "    calculate the corresponding PSLP as the average.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "         data to calculate PSLP for, must have datetime index\n",
    "    original_headers : list of str\n",
    "                       categories to calculate PSLP for\n",
    "    date_str : str\n",
    "               date 'YYYYMMDD' to calculate PSLP for; if None, calculate PSLP for all dates\n",
    "    lookback : int\n",
    "               number of days to consider in each category for calculating PSLP\n",
    "    country_code : str\n",
    "                   considered country (for holidays)\n",
    "    \"\"\"\n",
    "    unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "    df = assign_pslp_categories(df, country_code)\n",
    "\n",
    "    print(f\"Calculating PSLP for date {date_str}...\")\n",
    "    date = pd.to_datetime(date_str)\n",
    "    \n",
    "    pslp_category = get_pslp_category(date_str)\n",
    "    print(f\"PSLP category of {date.date()} is {pslp_category}.\")\n",
    "\n",
    "    # Check whether date is in range of given dataframe.\n",
    "    if date.date() < unique_dates[0]:\n",
    "        raise IndexError(f\"PSLP cannot be calculated. Date {date_str} is in the past.\")\n",
    "    if date.date()  > unique_dates[-1] + pd.Timedelta(days = 1) and date.date() != _get_nearest_future_pslp_date(date_str, pslp_category):\n",
    "        raise IndexError(f\"PSLP cannot be calculated. Date {date_str} is too far in the future.\")\n",
    "    assert date.date() in unique_dates\n",
    "    \n",
    "    unique_dates_pslp = df[df['PSLP Category'] == pslp_category].index.to_series().dt.date.drop_duplicates().tolist()\n",
    "    idx_pslp = unique_dates_pslp.index(date.date())\n",
    "    print(f\"Index in unique days of PSLP category is {idx_pslp}.\")\n",
    "    if idx_pslp - lookback < 0:\n",
    "        raise IndexError(f\"PSLP cannot be calculated. Less than {lookback} samples in PSLP category for date {date_str}.\")\n",
    "    lookback_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in unique_dates_pslp[idx_pslp-lookback:idx_pslp]]\n",
    "    print(f\"Dates to consider for calculating PSLP: {lookback_dates}\")\n",
    "    for header in original_headers:\n",
    "        print(f\"{header}...\")\n",
    "        df[header+\" PSLP\"].at[date_str] = pd.concat([df[header].at[d].reset_index(drop=True) for d in lookback_dates], axis=1).mean(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pslps(df, original_headers, date_str=None, lookback=3, country_code='DE', DEBUG=False):\n",
    "    \"\"\"\n",
    "    Calculate PSLPs for all dates in dataframe or for given date from given data.\n",
    "    \n",
    "    The data is categorized into weekdays, Saturdays, and Sundays/holidays.\n",
    "    The `lookback` most recent days from the specified date's category are used to\n",
    "    calculate the corresponding PSLP as the average.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.Dataframe\n",
    "         data to calculate PSLP for, must have datetime index\n",
    "    original_headers : list of str\n",
    "                       categories to calculate PSLP for\n",
    "    date_str : str\n",
    "               date 'YYYYMMDD' to calculate PSLP for; if None, calculate PSLP for all dates\n",
    "    lookback : int\n",
    "               number of days to consider in each category for calculating PSLP\n",
    "    country_code : str\n",
    "                   considered country (for holidays)\n",
    "    \"\"\"\n",
    "    if date_str is not None:\n",
    "        print(f\"Calculating PSLP for date {date_str} only...\")\n",
    "        df = _calculate_pslp(df, \n",
    "                            original_headers, \n",
    "                            date_str, \n",
    "                            lookback=lookback, \n",
    "                            country_code=country_code, \n",
    "                            DEBUG=DEBUG)\n",
    "    \n",
    "    else:\n",
    "        print(\"Calculating PSLPs for all dates in dataframe...\")\n",
    "        unique_dates = df.index.to_series().dt.date.drop_duplicates().tolist() # Get unique dates in data index.\n",
    "        unique_dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in unique_dates]\n",
    "        \n",
    "        for date in unique_dates:\n",
    "            try:\n",
    "                df = _calculate_pslp(df, \n",
    "                                    original_headers, \n",
    "                                    date_str=date, \n",
    "                                    lookback=lookback, \n",
    "                                    country_code=country_code, \n",
    "                                    DEBUG=DEBUG)\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_pslps(df_test, original_headers)#, date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d612dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.loc[\"20181028\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503dd067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_residuals(df, original_headers):\n",
    "    \"\"\"\n",
    "    Calculate residuals of actual data w.r.t PSLPs.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "         pre-processed data with PSLPs and residuals\n",
    "    original_headers : list of str\n",
    "                       original headers in ENTSO-E dataframe\n",
    "    \"\"\"\n",
    "    for header in original_headers:\n",
    "        df[header+\" Residuals\"] = df[header] - df[header+\" PSLP\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc59c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = calculate_residuals(df_test, original_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7af34b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test[\"Actual Load Residuals\"].loc[\"20180101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803f742",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_data(df, original_headers):\n",
    "    \"\"\"\n",
    "    Plot preprocessed load and generation data.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "         pre-processed data with PSLPs and residuals\n",
    "    original_headers : list of str\n",
    "                       original headers in ENTSO-E dataframe\n",
    "    \"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    num_rows = len(original_headers)\n",
    "    \n",
    "    fig = make_subplots(rows=num_rows, cols=1, subplot_titles=(original_headers))\n",
    "\n",
    "    for i, header in enumerate(original_headers):\n",
    "        fig.add_trace(go.Scatter(x = df.index, y = df[header], name=header), row=i+1, col=1)\n",
    "        fig.add_trace(go.Scatter(x = df.index, y = df[header+\" PSLP\"], name=header+\" PSLP\"), row=i+1, col=1)\n",
    "        fig.add_trace(go.Scatter(x = df.index, y = df[header+\" Residuals\"], name=header+\" Residuals\"), row=i+1, col=1)\n",
    "\n",
    "    fig.update_layout(height=10000, width=1200)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becaff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(df, original_headers):\n",
    "    \"\"\"\n",
    "    Calculate forecasting errors for preprocessed ENTSO-E load and generation data.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "         pre-processed data with PSLPs and residuals\n",
    "    original_headers : list of str\n",
    "                       original headers in ENTSO-E dataframe\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error    \n",
    "    for header in original_headers:\n",
    "        temp = pd.concat([df[header], df[header+\" PSLP\"]], axis=1).dropna()\n",
    "        mae = mean_absolute_error(temp[header], temp[header+\" PSLP\"])\n",
    "        mape = mean_absolute_percentage_error(temp[header], temp[header+\" PSLP\"])\n",
    "        mse = mean_squared_error(temp[header], temp[header+\" PSLP\"])\n",
    "        print_str = f\"{header}:\\n\"\n",
    "        print_str += f\"MAE = {mae}\\nMSE = {mse}\\nMAPE = {mape}\\n\"\n",
    "        print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_errors(df_test, original_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df_test, original_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dabc64",
   "metadata": {},
   "source": [
    "### Complete parameter list\n",
    "https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_complete_parameter_list\n",
    "\n",
    "### Queries returning Pandas Series\n",
    "\n",
    "`client.query_day_ahead_prices(country_code, start=start,end=end)` <br>\n",
    "`client.query_net_position(country_code, start=start, end=end, dayahead=True)` <br>\n",
    "`client.query_crossborder_flows(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_scheduled_exchanges(country_code_from, country_code_to, start, end, dayahead=False)` <br>\n",
    "`client.query_net_transfer_capacity_dayahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_weekahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_monthahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_yearahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_intraday_offered_capacity(country_code_from, country_code_to, start, end,implicit=True)` <br>\n",
    "`client.query_offered_capacity(country_code_from, country_code_to, start, end, contract_marketagreement_type, implicit=True)` <br>\n",
    "`client.query_aggregate_water_reservoirs_and_hydro_storage(country_code, start, end)`\n",
    "\n",
    "### Queries returning Pandas DataFrames\n",
    "\n",
    "`client.query_load(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_and_forecast(country_code, start=start, end=end)` <br>\n",
    "`client.query_generation_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_wind_and_solar_forecast(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation_per_plant(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity_per_unit(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_imbalance_prices(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_prices(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_amount(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_unavailability_of_generation_units(country_code, start=start,end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_transmission(country_code_from, country_code_to, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_withdrawn_unavailability_of_generation_units(country_code, start, end)` <br>\n",
    "`client.query_import(country_code, start, end)` <br>\n",
    "`client.query_generation_import(country_code, start, end)` <br>\n",
    "`client.query_procured_balancing_capacity(country_code, start, end, process_type, type_marketagreement_type=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c395d5",
   "metadata": {},
   "source": [
    "## Load data from client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"load forecast\"] = client.query_load_forecast(country_code, start=start,end=end)\n",
    "df[\"load\"] = client.query_load(country_code, start=start,end=end)\n",
    "df[\"load forecast error\"] = df[\"load forecast\"] - df[\"load\"]\n",
    "df[\"generation forecast\"] = client.query_generation_forecast(country_code, start=start,end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = client.query_generation(country_code, start=start,end=end, psr_type=None)\n",
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64521a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0978cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "ax.plot(df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation\"] = df_gen.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation forecast error\"] = df[\"generation forecast\"] - df[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b9c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87d813",
   "metadata": {},
   "source": [
    "## Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82506c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('entsoe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
