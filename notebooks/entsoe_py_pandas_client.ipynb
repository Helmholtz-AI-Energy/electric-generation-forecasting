{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22bc40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- How to deal with different columns in generation data for old (`DE_AT_LU` until 2018/09/30) and new bidding zone (`DE_LU` since 2018/10/01)? Old data contains all columns from new data but also additional columns, mostly about `'Actual Consumption'`, and one extra category `'Fossil Coal-derived gas Actual Aggregated'`.\n",
    "- Which time span to include in general for training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdcdbd",
   "metadata": {},
   "source": [
    "## Data-loading playground with `entsoe-py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9815c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a84800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa02137",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2013225",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6e68642c-8403-4caa-af31-bda40b8c67f6\" # web token for RESTful API\n",
    "country_code = \"10Y1001A1001A83F\" # Germany\n",
    "BZ_code = \"DE_LU\" # new bidding zone, valid since 2018/10/01\n",
    "BZ_code_old = \"DE_AT_LU\" # old bidding zone, valid until 2018/09/30\n",
    "time_zone = \"Europe/Berlin\" # time zone for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef278",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a15b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_diff_intersect(df1, df2):\n",
    "    \"\"\"\n",
    "    Return difference and intersection of columns of two dataframes.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    df1 : pandas.DataFrame\n",
    "          first dataframe\n",
    "    df2 : pandas.DataFrame\n",
    "          second dataframe\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    difference in columns of df1 and df2\n",
    "    intersection of columns of df1 and df2\n",
    "    \n",
    "    \"\"\"\n",
    "    return df1.columns.difference(df2.columns), df1.columns.intersection(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12004015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_intervals(start_date, \n",
    "                       end_date, \n",
    "                       time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Get time points for sequential data loading from ENTSO-E transparency platform.\n",
    "    \n",
    "    For one request, the time delta for loading data from the platform is limited to one year.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    pandas series with timestamps of time points to consider between start and end date\n",
    "    \"\"\"\n",
    "    # Convert start and end dates to timestamps.\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "\n",
    "    # Create series from start and end timestamps.\n",
    "    start_series = pd.Series(pd.Timestamp(start_date))\n",
    "    end_series = pd.Series(pd.Timestamp(end_date))\n",
    "    \n",
    "    # Create date range from start and end dates and determine year starts within range.\n",
    "    # Convert data range to series.\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=\"YS\", inclusive=\"both\").to_series()\n",
    "\n",
    "    # Check whether start date itself is year start.\n",
    "    # If not, prepend to dates to consider for data loading.\n",
    "    if not start.is_year_start:\n",
    "        dates = pd.concat([start_series, dates], ignore_index=True)\n",
    "\n",
    "    # Check whether end date itself is year start.\n",
    "    # If not, append to dates to consider for data loading.\n",
    "    if not end.is_year_start:\n",
    "        dates = pd.concat([dates, end_series], ignore_index=True)\n",
    "        \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaafbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(start_date, \n",
    "              end_date, \n",
    "              api_key, \n",
    "              country_code=\"10Y1001A1001A83F\", \n",
    "              time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Load actual load and actual aggregated gemeratopm per production type for requested time interval.\n",
    "    \n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "                \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with time points as indices and load + generation per type as columns.\n",
    "    \"\"\"\n",
    "    from entsoe import EntsoePandasClient\n",
    "    # Initialize client and settings.\n",
    "    client = EntsoePandasClient(api_key=api_key)\n",
    "    start = pd.Timestamp(start_date, tz=time_zone)\n",
    "    end = pd.Timestamp(end_date, tz=time_zone)\n",
    "    # Query data and save to dataframe.\n",
    "    df_load = client.query_load(country_code, start=start, end=end)\n",
    "    print(f\"Actual load has shape {df_load.shape}.\")\n",
    "    df_gen = client.query_generation(country_code, start=start, end=end, psr_type=None)\n",
    "    df_gen.columns = [\" \".join(a) for a in df_gen.columns.to_flat_index()]\n",
    "    print(f\"Actual generation per production type has shape {df_gen.shape}.\")\n",
    "    df_final = pd.concat([df_load, df_gen], axis=1) # Concatenate dataframes in columns dimension.\n",
    "    print(f\"Concatenated data frame has shape {df_final.shape}.\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822010db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start_date, \n",
    "               end_date, \n",
    "               api_key, \n",
    "               country_code=\"10Y1001A1001A83F\", \n",
    "               time_zone=\"Europe/Berlin\"):\n",
    "    \"\"\"\n",
    "    Fetch data from ENTSO-E transparency platform as requested.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : str\n",
    "                 start date as \"yyyymmdd\"\n",
    "    end_date : str\n",
    "               end date as \"yyyymmdd\"\n",
    "    api_key : str\n",
    "              RESTful API web key\n",
    "    time_zone : str\n",
    "                time zone as string, e.g. \"Europe/Berlin\"\n",
    "    country_code : str\n",
    "                   code for country, bidding zone, etc.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with actual load and generation per type for requested time interval\n",
    "    \"\"\"\n",
    "    # Determine sequence of dates to consider when loading data.\n",
    "    dates = get_load_intervals(start_date, end_date, time_zone)\n",
    "    print(f\"Consider the following dates:\\n{dates}\")\n",
    "    df_list = []\n",
    "    \n",
    "    for i, _ in enumerate(dates):\n",
    "\n",
    "        if i == dates.shape[0] - 1:\n",
    "            print(\"Returning final data frame...\")\n",
    "            return pd.concat(df_list, axis=0) # Concatenate dataframes along time axis (index).\n",
    "            \n",
    "        try:\n",
    "            print(f\"Trying to load data chunk for time interval [{dates[i]}, {dates[i+1]}]...\")\n",
    "            df_temp = load_data(start_date=dates[i], \n",
    "                                end_date=dates[i+1],\n",
    "                                api_key=api_key,\n",
    "                                time_zone=time_zone,\n",
    "                                country_code=country_code)\n",
    "            print(df_temp.shape)\n",
    "            df_list.append(df_temp)\n",
    "            print(\"Loading successful!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Loading failed!\", e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3415492a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following dates:\n",
      "2014-01-01   2014-01-01\n",
      "2015-01-01   2015-01-01\n",
      "2016-01-01   2016-01-01\n",
      "2017-01-01   2017-01-01\n",
      "2018-01-01   2018-01-01\n",
      "2019-01-01   2019-01-01\n",
      "2020-01-01   2020-01-01\n",
      "2021-01-01   2021-01-01\n",
      "2022-01-01   2022-01-01\n",
      "2023-01-01   2023-01-01\n",
      "Freq: AS-JAN, dtype: datetime64[ns]\n",
      "Trying to load data chunk for time interval [2014-01-01 00:00:00, 2015-01-01 00:00:00]...\n",
      "Loading failed! \n",
      "Trying to load data chunk for time interval [2015-01-01 00:00:00, 2016-01-01 00:00:00]...\n",
      "Actual load has shape (35040, 1).\n",
      "Actual generation per production type has shape (35040, 33).\n",
      "Concatenated data frame has shape (35040, 34).\n",
      "(35040, 34)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2016-01-01 00:00:00, 2017-01-01 00:00:00]...\n",
      "Actual load has shape (35136, 1).\n",
      "Actual generation per production type has shape (35136, 33).\n",
      "Concatenated data frame has shape (35136, 34).\n",
      "(35136, 34)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2017-01-01 00:00:00, 2018-01-01 00:00:00]...\n",
      "Actual load has shape (35040, 1).\n",
      "Actual generation per production type has shape (35040, 33).\n",
      "Concatenated data frame has shape (35040, 34).\n",
      "(35040, 34)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2018-01-01 00:00:00, 2019-01-01 00:00:00]...\n",
      "Actual load has shape (35040, 1).\n",
      "Actual generation per production type has shape (35040, 25).\n",
      "Concatenated data frame has shape (35040, 26).\n",
      "(35040, 26)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2019-01-01 00:00:00, 2020-01-01 00:00:00]...\n",
      "Actual load has shape (35040, 1).\n",
      "Actual generation per production type has shape (35040, 22).\n",
      "Concatenated data frame has shape (35040, 23).\n",
      "(35040, 23)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2020-01-01 00:00:00, 2021-01-01 00:00:00]...\n",
      "Actual load has shape (35136, 1).\n",
      "Actual generation per production type has shape (35136, 21).\n",
      "Concatenated data frame has shape (35136, 22).\n",
      "(35136, 22)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2021-01-01 00:00:00, 2022-01-01 00:00:00]...\n",
      "Actual load has shape (35040, 1).\n",
      "Actual generation per production type has shape (35040, 22).\n",
      "Concatenated data frame has shape (35040, 23).\n",
      "(35040, 23)\n",
      "Loading successful!\n",
      "Trying to load data chunk for time interval [2022-01-01 00:00:00, 2023-01-01 00:00:00]...\n",
      "Actual load has shape (34037, 1).\n",
      "Actual generation per production type has shape (34127, 19).\n",
      "Concatenated data frame has shape (34127, 20).\n",
      "(34127, 20)\n",
      "Loading successful!\n",
      "Returning final data frame...\n"
     ]
    }
   ],
   "source": [
    "start_date = \"20140101\"\n",
    "end_date = \"20230101\"\n",
    "df_test = fetch_data(start_date, end_date, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86820446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279599, 34)\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test[\"Actual Load\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dabc64",
   "metadata": {},
   "source": [
    "### Complete parameter list\n",
    "https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_complete_parameter_list\n",
    "\n",
    "### Queries returning Pandas Series\n",
    "\n",
    "`client.query_day_ahead_prices(country_code, start=start,end=end)` <br>\n",
    "`client.query_net_position(country_code, start=start, end=end, dayahead=True)` <br>\n",
    "`client.query_crossborder_flows(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_scheduled_exchanges(country_code_from, country_code_to, start, end, dayahead=False)` <br>\n",
    "`client.query_net_transfer_capacity_dayahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_weekahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_monthahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_net_transfer_capacity_yearahead(country_code_from, country_code_to, start, end)` <br>\n",
    "`client.query_intraday_offered_capacity(country_code_from, country_code_to, start, end,implicit=True)` <br>\n",
    "`client.query_offered_capacity(country_code_from, country_code_to, start, end, contract_marketagreement_type, implicit=True)` <br>\n",
    "`client.query_aggregate_water_reservoirs_and_hydro_storage(country_code, start, end)`\n",
    "\n",
    "### Queries returning Pandas DataFrames\n",
    "\n",
    "`client.query_load(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_load_and_forecast(country_code, start=start, end=end)` <br>\n",
    "`client.query_generation_forecast(country_code, start=start,end=end)` <br>\n",
    "`client.query_wind_and_solar_forecast(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_generation_per_plant(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_installed_generation_capacity_per_unit(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_imbalance_prices(country_code, start=start,end=end, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_prices(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_contracted_reserve_amount(country_code, start, end, type_marketagreement_type, psr_type=None)` <br>\n",
    "`client.query_unavailability_of_generation_units(country_code, start=start,end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_unavailability_transmission(country_code_from, country_code_to, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)` <br>\n",
    "`client.query_withdrawn_unavailability_of_generation_units(country_code, start, end)` <br>\n",
    "`client.query_import(country_code, start, end)` <br>\n",
    "`client.query_generation_import(country_code, start, end)` <br>\n",
    "`client.query_procured_balancing_capacity(country_code, start, end, process_type, type_marketagreement_type=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c395d5",
   "metadata": {},
   "source": [
    "## Load data from client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"load forecast\"] = client.query_load_forecast(country_code, start=start,end=end)\n",
    "df[\"load\"] = client.query_load(country_code, start=start,end=end)\n",
    "df[\"load forecast error\"] = df[\"load forecast\"] - df[\"load\"]\n",
    "df[\"generation forecast\"] = client.query_generation_forecast(country_code, start=start,end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = client.query_generation(country_code, start=start,end=end, psr_type=None)\n",
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64521a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0978cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "ax.plot(df_gen.xs(key=\"Actual Aggregated\", level=1, axis=1))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation\"] = df_gen.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation forecast error\"] = df[\"generation forecast\"] - df[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b9c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87d813",
   "metadata": {},
   "source": [
    "## Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82506c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('entsoe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
