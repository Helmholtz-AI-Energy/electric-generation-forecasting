{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96058203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import plotly.express as px\n",
    "from pmdarima.arima.utils import ndiffs, nsdiffs\n",
    "from pmdarima.arima import auto_arima\n",
    "from pmdarima import model_selection\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7069e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6e68642c-8403-4caa-af31-bda40b8c67f6\" # web token for RESTful API\n",
    "country_code = \"10Y1001A1001A83F\" # Germany\n",
    "time_zone = \"Europe/Berlin\" # time zone for Germany\n",
    "start_date = \"20171225\"\n",
    "end_date = \"20180102\"\n",
    "downsample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343a5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARIMADataset:\n",
    "    \"\"\"\n",
    "    Fetch and preprocess ENTSO-E load and generation data. Caluclate SARIMA predictions and residuals.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_date, \n",
    "        end_date,                          # overall end date\n",
    "        api_key, # web token for RESTful API\n",
    "        country_code = \"10Y1001A1001A83F\", # country code (default: Germany)\n",
    "        time_zone = \"Europe/Berlin\", # time zone for Germany\n",
    "        downsample = False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize SARIMA dataset.\n",
    "        Params\n",
    "        ------\n",
    "        start_date : str\n",
    "                     overall start date as \"YYYYMMDD\"\n",
    "        end_date : str\n",
    "                   overall end date as \"YYYYMMDD\"\n",
    "        api_key : str\n",
    "                  web token for RESTfulAPI access to ENTSO-E transparency platform\n",
    "        country_code : str\n",
    "                       country code\n",
    "        time zone : str\n",
    "                    time zone\n",
    "        downsample : bool\n",
    "                     Downsample to 1h resolution or not.\n",
    "        \"\"\"\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.country = country_code\n",
    "        self.time_zone = time_zone\n",
    "        self.api_key = api_key\n",
    "        self.df = None\n",
    "        self.original_headers = None\n",
    "        self.errors = None\n",
    "        self.fetch_data()\n",
    "        #self.calculate_pslps()\n",
    "        #self.calculate_sarima()\n",
    "        #self.calculate_errors()\n",
    "    \n",
    "    def _get_load_intervals(self):\n",
    "        \"\"\"\n",
    "        Get time points for sequential data loading from ENTSO-E transparency platform.\n",
    "        \n",
    "        For one request, the time delta for loading data from the platform is limited to one year.\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "        pandas series with timestamps of time points to consider between start and end date\n",
    "        \"\"\"\n",
    "        # Convert start and end dates to timestamps.\n",
    "        start = pd.Timestamp(self.start_date, tz=self.time_zone)\n",
    "        end = pd.Timestamp(self.end_date, tz=self.time_zone)\n",
    "    \n",
    "        # Create series from start and end timestamps.\n",
    "        start_series = pd.Series(pd.Timestamp(self.start_date))\n",
    "        end_series = pd.Series(pd.Timestamp(self.end_date))\n",
    "        \n",
    "        # Create date range from start and end dates and determine year starts within range.\n",
    "        # Convert data range to series.\n",
    "        dates = pd.date_range(start=self.start_date, end=self.end_date, freq=\"YS\", inclusive=\"both\").to_series()\n",
    "    \n",
    "        # Check whether start date itself is year start.\n",
    "        # If not, prepend to dates to consider for data loading.\n",
    "        if not start.is_year_start:\n",
    "            dates = pd.concat([start_series, dates], ignore_index=True)\n",
    "    \n",
    "        # Check whether end date itself is year start.\n",
    "        # If not, append to dates to consider for data loading.\n",
    "        if not end.is_year_start:\n",
    "            dates = pd.concat([dates, end_series], ignore_index=True)\n",
    "            \n",
    "        return dates\n",
    "        \n",
    "        \n",
    "    def _load_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Load actual load and actual aggregated generation per production type for requested time interval.\n",
    "        Params\n",
    "        ------\n",
    "        start_date : str\n",
    "                     start date as \"yyyymmdd\"\n",
    "        end_date : str\n",
    "                   end date as \"yyyymmdd\"\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame with time points as indices and load + generation per type as columns.\n",
    "        \"\"\"\n",
    "        from entsoe import EntsoePandasClient\n",
    "        # Initialize client and settings.\n",
    "        client = EntsoePandasClient(api_key=self.api_key)\n",
    "        start = pd.Timestamp(start_date, tz=self.time_zone)\n",
    "        end = pd.Timestamp(end_date, tz=self.time_zone)\n",
    "        # Query data and save to dataframe.\n",
    "        df_load = client.query_load(self.country, start=start, end=end)\n",
    "        print(f\"Actual load has shape {df_load.shape}.\")\n",
    "        df_gen = client.query_generation(self.country, start=start, end=end, psr_type=None)\n",
    "        df_gen.columns = [\" \".join(a) for a in df_gen.columns.to_flat_index()]\n",
    "        print(f\"Actual generation per production type has shape {df_gen.shape}.\")\n",
    "        df_final = pd.concat([df_load, df_gen], axis=1) # Concatenate dataframes in columns dimension.\n",
    "        print(f\"Concatenated data frame has shape {df_final.shape}.\")\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    \n",
    "    def fetch_data(self, drop_consumption=True):\n",
    "        \"\"\"\n",
    "        Fetch actual load and generation per type from ENTSO-E transparency platform \n",
    "        for requested time interval. Set resulting dataframe as attribute.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        drop_consumption : Bool\n",
    "                           Drop columns containing actual consumption.\n",
    "        \"\"\"\n",
    "        # Determine sequence of dates to consider when loading data.\n",
    "        dates = self._get_load_intervals()\n",
    "        print(f\"Consider the following dates:\\n{dates}\")\n",
    "        df_list = []\n",
    "        \n",
    "        for i, _ in enumerate(dates):\n",
    "    \n",
    "            if i == dates.shape[0] - 1:\n",
    "                df_final = pd.concat(df_list, axis=0) # Concatenate dataframes along time axis (index).\n",
    "                df_final.index = pd.to_datetime(df_final.index, utc=True).tz_convert(tz=\"UTC+01:00\")\n",
    "    \n",
    "                # Drop columns containing actual consumption?\n",
    "                if drop_consumption:\n",
    "                    print(\"Dropping columns containing actual consumption...\")\n",
    "                    df_final.drop(list(df_final.filter(regex='Consumption')), axis=1, inplace=True)\n",
    "                original_headers = df_final.columns\n",
    "\n",
    "                print(\"Creating columns for PSLP calculation...\")\n",
    "                for header in original_headers:\n",
    "                    df_final[str(header) + \" PSLP\"] = pd.Series(dtype='float')\n",
    "                if downsample:\n",
    "                    print(\"Downsample to 1h resolution...\")\n",
    "                    df_final = df_final.resample('1H', axis='index').mean()\n",
    "                print(\"Returning final data frame...\")\n",
    "                self.df = df_final\n",
    "                self.original_headers = original_headers\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                print(f\"Trying to load data chunk for time interval [{dates[i]}, {dates[i+1]}]...\")\n",
    "                df_temp = self._load_data(start_date=dates[i], end_date=dates[i+1])\n",
    "                print(df_temp.shape)\n",
    "                df_list.append(df_temp)\n",
    "                print(\"Loading successful!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Loading failed!\", e)\n",
    "                continue\n",
    "\n",
    "    @staticmethod            \n",
    "    def get_pslp_category(date, weekday=None, holiday=None, country_code='DE'):\n",
    "        \"\"\"\n",
    "        Get PSLP category from date, weekday information, and holiday information.\n",
    "        0 : weekday\n",
    "        1 : Saturday\n",
    "        2 : Sunday and holiday\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        date : str\n",
    "               date in 'YYYYMMDD' format\n",
    "        weekday : int\n",
    "                  corresponding weekday\n",
    "                  0 - Mon, 1 - Tue, 2 - Wed, 3 - Thu, 4 - Fri, 5 - Sat, 6 - Sun\n",
    "        holiday : Bool\n",
    "                  True if public holiday, False if not.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int : PSLP category\n",
    "        \"\"\"\n",
    "        # Convert string-type date to datetime object.\n",
    "        if type(date) is str:\n",
    "            date = pd.to_datetime(date)\n",
    "        \n",
    "        # Assign weekday if not given.\n",
    "        if weekday is None:\n",
    "            weekday = date.weekday()\n",
    "        \n",
    "        # Assign holiday category if not given.\n",
    "        if holiday is None:\n",
    "            import holidays\n",
    "            holiday = date in holidays.country_holidays(country_code)\n",
    "        \n",
    "        # Special treatment for Christmas eve and New year's eve as Saturdays.\n",
    "        if ( date.day == 24 or date.day == 31 ) and date.month == 12 and weekday != 6:\n",
    "            pslp_category = 1\n",
    "        # weekdays\n",
    "        elif weekday < 5 and holiday is False:\n",
    "            pslp_category = 0\n",
    "        # Saturdays\n",
    "        elif weekday == 5 and holiday is False:\n",
    "            pslp_category = 1\n",
    "        # Sundays and holidays\n",
    "        elif weekday == 6 or holiday is True:\n",
    "            pslp_category = 2\n",
    "        return pslp_category\n",
    "    \n",
    "    \n",
    "    def _assign_pslp_categories(self, country_code=\"DE\"):\n",
    "        \"\"\"\n",
    "        Assign PSLP categories to dates in dataframe's datetime index.\n",
    "    \n",
    "        0 is weekday, 1 is Saturday, 2 is Sunday or holiday.\n",
    "        Special treatment for Christmas eve and New Year's eve (as Saturdays).\n",
    "    \n",
    "        Params\n",
    "        ------\n",
    "        df : pandas.Dataframe\n",
    "        country_code : str\n",
    "                       country to determine holidays for\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Dataframe\n",
    "        Dataframe amended by weekday information, holiday information, and PSLP category\n",
    "        \"\"\"\n",
    "    \n",
    "        import holidays\n",
    "        \n",
    "        # Get holidays in specified country.\n",
    "        country_holidays = holidays.country_holidays(country_code) # Passing a state is also possible!\n",
    "    \n",
    "        s = self.df.index.to_series()                                # Convert datetime index to series.\n",
    "        dates = s.dt.date                                       # Get plain dates from datetime objects.\n",
    "        weekdays = s.dt.weekday                                 # Get weekdays from datetime objects.\n",
    "        holidays = [date in country_holidays for date in dates] # Determine holidays.\n",
    "        pslp_category = []\n",
    "        \n",
    "        for d, wd, hd in zip(dates, weekdays, holidays):\n",
    "            pslp_category.append(self.get_pslp_category(d, wd, hd))\n",
    "            \n",
    "        self.df[\"PSLP Category\"] = pslp_category\n",
    "        self.df[\"Holiday\"] = holidays\n",
    "        self.df[\"Weekday\"] = weekdays\n",
    "    \n",
    "    \n",
    "    def calculate_residuals(self):\n",
    "        \"\"\"\n",
    "        Calculate residuals of actual data w.r.t PSLPs.\n",
    "        \"\"\"\n",
    "        for header in self.original_headers:\n",
    "            self.df[header+\" Residuals\"] = self.df[header] - self.df[header+\" SARIMA\"]\n",
    "    \n",
    "    \n",
    "    def plot_data(self):\n",
    "        \"\"\"\n",
    "        Plot preprocessed load and generation data.\n",
    "        \"\"\"\n",
    "        from plotly.subplots import make_subplots\n",
    "        import plotly.graph_objects as go\n",
    "    \n",
    "        num_rows = len(self.original_headers)\n",
    "        \n",
    "        fig = make_subplots(rows=num_rows, cols=1, subplot_titles=(self.original_headers))\n",
    "    \n",
    "        for i, header in enumerate(self.original_headers):\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header], name=header), row=i+1, col=1)\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header+\" PSLP\"], name=header+\" PSLP\"), row=i+1, col=1)\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header+\" Residuals\"], name=header+\" Residuals\"), row=i+1, col=1)\n",
    "    \n",
    "        fig.update_layout(height=10000, width=1200)\n",
    "        fig.show()\n",
    "        \n",
    "       \n",
    "    def calculate_errors(self):\n",
    "        \"\"\"\n",
    "        Calculate forecasting errors for preprocessed ENTSO-E load and generation data.  \n",
    "        \"\"\"\n",
    "        from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "        errors = {}\n",
    "        for header in self.original_headers:\n",
    "            temp = pd.concat([self.df[header], self.df[header+\" PSLP\"]], axis=1).dropna()\n",
    "            mae = mean_absolute_error(temp[header], temp[header+\" PSLP\"])\n",
    "            mape = mean_absolute_percentage_error(temp[header], temp[header+\" PSLP\"])\n",
    "            mse = mean_squared_error(temp[header], temp[header+\" PSLP\"])\n",
    "            errors[header] = {\"MAE\": mae, \"MAPE\": mape, \"MSE\": mse}\n",
    "        self.errors = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7251161f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the following dates:\n",
      "0   2017-12-25\n",
      "1   2018-01-01\n",
      "2   2018-01-02\n",
      "dtype: datetime64[ns]\n",
      "Trying to load data chunk for time interval [2017-12-25 00:00:00, 2018-01-01 00:00:00]...\n",
      "Loading failed! 503 Server Error: Service Unavailable for url: https://web-api.tp.entsoe.eu/api?documentType=A65&processType=A16&outBiddingZone_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&securityToken=6e68642c-8403-4caa-af31-bda40b8c67f6&periodStart=201712242300&periodEnd=201712312300\n",
      "Trying to load data chunk for time interval [2018-01-01 00:00:00, 2018-01-02 00:00:00]...\n",
      "Loading failed! 503 Server Error: Service Unavailable for url: https://web-api.tp.entsoe.eu/api?documentType=A65&processType=A16&outBiddingZone_Domain=10Y1001A1001A83F&out_Domain=10Y1001A1001A83F&securityToken=6e68642c-8403-4caa-af31-bda40b8c67f6&periodStart=201712312300&periodEnd=201801012300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SarimaData \u001b[38;5;241m=\u001b[39m \u001b[43mSARIMADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_zone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mSARIMADataset.__init__\u001b[0;34m(self, start_date, end_date, api_key, country_code, time_zone, downsample)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m, in \u001b[0;36mSARIMADataset.fetch_data\u001b[0;34m(self, drop_consumption)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dates):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m dates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m         df_final \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Concatenate dataframes along time axis (index).\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         df_final\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_final\u001b[38;5;241m.\u001b[39mindex, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtz_convert(tz\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC+01:00\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;66;03m# Drop columns containing actual consumption?\u001b[39;00m\n",
      "File \u001b[0;32m/hkfs/home/project/hk-project-test-mss/ku4408/.virtualenvs/entsoe/lib64/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hkfs/home/project/hk-project-test-mss/ku4408/.virtualenvs/entsoe/lib64/python3.9/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/hkfs/home/project/hk-project-test-mss/ku4408/.virtualenvs/entsoe/lib64/python3.9/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "SarimaData = SARIMADataset(start_date, end_date, api_key, country_code, time_zone, downsample=downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelations(data):\n",
    "    \"\"\"Plot ACF and PACF for given data.\"\"\"\n",
    "    plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n",
    "    # Original series\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    axes[0, 0].plot(data.to_numpy()); axes[0, 0].set_title(f'Original Series')\n",
    "    plot_acf(data.to_numpy(), ax=axes[0, 1])\n",
    "    plot_pacf(data.to_numpy(), ax=axes[0, 2], method='ywm')\n",
    "\n",
    "    # 1st-order differencing\n",
    "    axes[1, 0].plot(data.diff().to_numpy()); axes[1, 0].set_title(f'1st-Order Differencing')\n",
    "    plot_acf(data.diff().dropna().to_numpy(), ax=axes[1, 1])\n",
    "    plot_pacf(data.to_numpy(), ax=axes[1, 2], method='ywm')\n",
    "\n",
    "    # 2nd-order differencing\n",
    "    axes[2, 0].plot(data.diff().diff().to_numpy()); axes[2, 0].set_title(f'2nd-Order Differencing')\n",
    "    plot_acf(data.diff().diff().dropna().to_numpy(), ax=axes[2, 1])\n",
    "    plot_pacf(data.to_numpy(), ax=axes[2, 2], method='ywm')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e300f-0bb6-4e12-acc2-a365c35485a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = SarimaData.df[\"Actual Load\"].loc[\"2018-01-01\"]\n",
    "plot_autocorrelations(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d7537-ad32-499b-87e8-b71f26831cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_fraction=0.75, seasonal_cycle=96):\n",
    "    \"\"\"\n",
    "    Perform season-based train-test split of input data.\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "         overall dataset\n",
    "    train_fraction : float\n",
    "                     fraction of data to use for training\n",
    "    seasonal_cycle : int\n",
    "                     number of data points in each season\n",
    "                     Default 96 corresponds to 15min time resolution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_train : pandas.DataFrame\n",
    "               train dataset\n",
    "    df_test : pandas.DataFrame\n",
    "              test dataset\n",
    "    \"\"\"\n",
    "    overall_days = int(SarimaData.df.shape[0] / seasonal_cycle)\n",
    "    train_days = int(train_fraction * overall_days)\n",
    "    test_days = overall_days - train_days\n",
    "    return model_selection.train_test_split(df, train_size=train_days, test_size=test_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_cycle = 96\n",
    "train_fraction = 0.75\n",
    "data = SarimaData.df[\"Actual Load\"]\n",
    "data_train, data_test = split_train_test(data, train_fraction=train_fraction, seasonal_cycle=seasonal_cycle)\n",
    "print(\"Pre-compute (seasonal) differencing order to accelerate auto-ARIMA...\")\n",
    "# Estimate order of differencing d by performing a stationarity test for different d's. \n",
    "# Selects max. value d for which time series is judged stationary by statistical test.\n",
    "# Default unit root test of stationarity: Kwiatkowski–Phillips–Schmidt–Shin (KPSS)\n",
    "d = ndiffs(data, test='kpss')\n",
    "# Estimate order of seasonal differencing D by performing stationarity test of seasonality for different D's. \n",
    "# Selects max. value D for which time series is judged seasonally stationary by statistical test.\n",
    "# Default unit root test of stationarity: Osborn-Chui-Smith-Birchenhall (OCSB)\n",
    "D =b nsdiffs(data, m=seasonal_cycle, test='ocsb')\n",
    "print(f\"Differencing order is {d}. Seasonal differencing order is {D}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c9018-bfa6-4e46-8eaa-b320b91d6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(SarimaData.df.head())\n",
    "print(SarimaData.df.resample('1H', axis='index').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ff274-5998-451d-ae1a-e90870f2816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Automatically discover optimal order for SARIMAX model with `auto_arima`...\")\n",
    "start_p = 1\n",
    "start_q = 1\n",
    "max_p = 3\n",
    "max_q = 3\n",
    "start_P = 1\n",
    "start_Q= 1\n",
    "max_P = 2\n",
    "max_Q = 2\n",
    "model = auto_arima(data.values, \n",
    "                   start_p=start_p, \n",
    "                   d=d, \n",
    "                   start_q=start_q,\n",
    "                   max_p=max_p,\n",
    "                   max_q=max_q,\n",
    "                   start_P=start_P,\n",
    "                   start_Q=start_Q,\n",
    "                   max_P=max_P,\n",
    "                   max_Q=max_Q,\n",
    "                   D=D, \n",
    "                   m=seasonal_cycle,\n",
    "                   trace=True\n",
    "                  )\n",
    "print(\"DONE.\")\n",
    "print(model)\n",
    "print(type(model))\n",
    "model.summary()\n",
    "\n",
    "with open('arima.pkl', 'wb') as pkl:\n",
    "    pickle.dump(model, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcd0f9-8767-4e65-a214-12ac56795b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)\n",
    "model.summary()\n",
    "# Serialize model.\n",
    "with open('sarima.pkl', 'wb') as pkl:\n",
    "    pickle.dump(model, pkl)\n",
    "\n",
    "# You can still make predictions from the model at this point\n",
    "model.predict(n_periods=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d0773-5c9a-40de-bb56-f178d02da48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arima.pkl', 'rb') as pkl:\n",
    "    pickle_preds = pickle.load(pkl).predict(n_periods=5)\n",
    "print(pickle_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac3f43-0441-4ddb-9dba-12934cad4123",
   "metadata": {},
   "source": [
    "## Refreshing your SARIMAX models\n",
    "There are two ways to keep models up to date with `pmdarima`:\n",
    "1. Periodically, a model will need to be refreshed given new observations. One can either re-use `auto-arime`-estimated order terms or re-fitting altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in SarimaData.original_headers:\n",
    "    # Original data.\n",
    "    data = SarimaData.df[header]\n",
    "    dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "    # Get train fraction from original data.\n",
    "    data_train = SarimaData.df[header].head(train_days * seasonal_cycle)\n",
    "    dates_train = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data_train.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "    # Get test fraction from original data.\n",
    "    data_test = SarimaData.df[header].tail(test_days * seasonal_cycle)\n",
    "    dates_test = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data_test.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "    \n",
    "    auto_arima(data.values, m=seasonal_cycle, seasonal=True, stationary=False, information_criterion='aic', test='kpss', seasonal_test='ocsb', trend=None, method='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150eaa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entsoe",
   "language": "python",
   "name": "entsoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
