{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96058203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import plotly.express as px\n",
    "from math import sqrt\n",
    "#from pmdarima.arima.utils import ndiffs\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7069e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6e68642c-8403-4caa-af31-bda40b8c67f6\" # web token for RESTful API\n",
    "country_code = \"10Y1001A1001A83F\" # Germany\n",
    "time_zone = \"Europe/Berlin\" # time zone for Germany\n",
    "start_date = \"20171225\"\n",
    "end_date = \"20180102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARIMADataset:\n",
    "    \"\"\"\n",
    "    Fetch and preprocess ENTSO-E load and generation data. Caluclate SARIMA predictions and residuals.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_date, \n",
    "        end_date,                          # overall end date\n",
    "        api_key, # web token for RESTful API\n",
    "        country_code = \"10Y1001A1001A83F\", # country code (default: Germany)\n",
    "        time_zone = \"Europe/Berlin\" # time zone for Germany\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize SARIMA dataset.\n",
    "        Params\n",
    "        ------\n",
    "        start_date : str\n",
    "                     overall start date as \"YYYYMMDD\"\n",
    "        end_date : str\n",
    "                   overall end date as \"YYYYMMDD\"\n",
    "        api_key : str\n",
    "                  web token for RESTfulAPI access to ENTSO-E transparency platform\n",
    "        country_code : str\n",
    "                       country code\n",
    "        time zone : str\n",
    "                    time zone\n",
    "        \"\"\"\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.country = country_code\n",
    "        self.time_zone = time_zone\n",
    "        self.api_key = api_key\n",
    "        self.df = None\n",
    "        self.original_headers = None\n",
    "        self.errors = None\n",
    "        self.fetch_data()\n",
    "        #self.calculate_pslps()\n",
    "        #self.calculate_sarima()\n",
    "        #self.calculate_errors()\n",
    "    \n",
    "    def _get_load_intervals(self):\n",
    "        \"\"\"\n",
    "        Get time points for sequential data loading from ENTSO-E transparency platform.\n",
    "        \n",
    "        For one request, the time delta for loading data from the platform is limited to one year.\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "        pandas series with timestamps of time points to consider between start and end date\n",
    "        \"\"\"\n",
    "        # Convert start and end dates to timestamps.\n",
    "        start = pd.Timestamp(self.start_date, tz=self.time_zone)\n",
    "        end = pd.Timestamp(self.end_date, tz=self.time_zone)\n",
    "    \n",
    "        # Create series from start and end timestamps.\n",
    "        start_series = pd.Series(pd.Timestamp(self.start_date))\n",
    "        end_series = pd.Series(pd.Timestamp(self.end_date))\n",
    "        \n",
    "        # Create date range from start and end dates and determine year starts within range.\n",
    "        # Convert data range to series.\n",
    "        dates = pd.date_range(start=self.start_date, end=self.end_date, freq=\"YS\", inclusive=\"both\").to_series()\n",
    "    \n",
    "        # Check whether start date itself is year start.\n",
    "        # If not, prepend to dates to consider for data loading.\n",
    "        if not start.is_year_start:\n",
    "            dates = pd.concat([start_series, dates], ignore_index=True)\n",
    "    \n",
    "        # Check whether end date itself is year start.\n",
    "        # If not, append to dates to consider for data loading.\n",
    "        if not end.is_year_start:\n",
    "            dates = pd.concat([dates, end_series], ignore_index=True)\n",
    "            \n",
    "        return dates\n",
    "        \n",
    "        \n",
    "    def _load_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Load actual load and actual aggregated generation per production type for requested time interval.\n",
    "        Params\n",
    "        ------\n",
    "        start_date : str\n",
    "                     start date as \"yyyymmdd\"\n",
    "        end_date : str\n",
    "                   end date as \"yyyymmdd\"\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame with time points as indices and load + generation per type as columns.\n",
    "        \"\"\"\n",
    "        from entsoe import EntsoePandasClient\n",
    "        # Initialize client and settings.\n",
    "        client = EntsoePandasClient(api_key=self.api_key)\n",
    "        start = pd.Timestamp(start_date, tz=self.time_zone)\n",
    "        end = pd.Timestamp(end_date, tz=self.time_zone)\n",
    "        # Query data and save to dataframe.\n",
    "        df_load = client.query_load(self.country, start=start, end=end)\n",
    "        print(f\"Actual load has shape {df_load.shape}.\")\n",
    "        df_gen = client.query_generation(self.country, start=start, end=end, psr_type=None)\n",
    "        df_gen.columns = [\" \".join(a) for a in df_gen.columns.to_flat_index()]\n",
    "        print(f\"Actual generation per production type has shape {df_gen.shape}.\")\n",
    "        df_final = pd.concat([df_load, df_gen], axis=1) # Concatenate dataframes in columns dimension.\n",
    "        print(f\"Concatenated data frame has shape {df_final.shape}.\")\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    \n",
    "    def fetch_data(self, drop_consumption=True):\n",
    "        \"\"\"\n",
    "        Fetch actual load and generation per type from ENTSO-E transparency platform \n",
    "        for requested time interval. Set resulting dataframe as attribute.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        drop_consumption : Bool\n",
    "                           Drop columns containing actual consumption.\n",
    "        \"\"\"\n",
    "        # Determine sequence of dates to consider when loading data.\n",
    "        dates = self._get_load_intervals()\n",
    "        print(f\"Consider the following dates:\\n{dates}\")\n",
    "        df_list = []\n",
    "        \n",
    "        for i, _ in enumerate(dates):\n",
    "    \n",
    "            if i == dates.shape[0] - 1:\n",
    "                df_final = pd.concat(df_list, axis=0) # Concatenate dataframes along time axis (index).\n",
    "                df_final.index = pd.to_datetime(df_final.index, utc=True).tz_convert(tz=\"UTC+01:00\")\n",
    "    \n",
    "                # Drop columns containing actual consumption?\n",
    "                if drop_consumption:\n",
    "                    print(\"Dropping columns containing actual consumption...\")\n",
    "                    df_final.drop(list(df_final.filter(regex='Consumption')), axis=1, inplace=True)\n",
    "                original_headers = df_final.columns\n",
    "\n",
    "                print(\"Creating columns for PSLP calculation...\")\n",
    "                for header in original_headers:\n",
    "                    df_final[str(header) + \" PSLP\"] = pd.Series(dtype='float')\n",
    "                \n",
    "                print(\"Returning final data frame...\")\n",
    "                self.df = df_final\n",
    "                self.original_headers = original_headers\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                print(f\"Trying to load data chunk for time interval [{dates[i]}, {dates[i+1]}]...\")\n",
    "                df_temp = self._load_data(start_date=dates[i], end_date=dates[i+1])\n",
    "                print(df_temp.shape)\n",
    "                df_list.append(df_temp)\n",
    "                print(\"Loading successful!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Loading failed!\", e)\n",
    "                continue\n",
    "\n",
    "    @staticmethod            \n",
    "    def get_pslp_category(date, weekday=None, holiday=None, country_code='DE'):\n",
    "        \"\"\"\n",
    "        Get PSLP category from date, weekday information, and holiday information.\n",
    "        0 : weekday\n",
    "        1 : Saturday\n",
    "        2 : Sunday and holiday\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        date : str\n",
    "               date in 'YYYYMMDD' format\n",
    "        weekday : int\n",
    "                  corresponding weekday\n",
    "                  0 - Mon, 1 - Tue, 2 - Wed, 3 - Thu, 4 - Fri, 5 - Sat, 6 - Sun\n",
    "        holiday : Bool\n",
    "                  True if public holiday, False if not.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int : PSLP category\n",
    "        \"\"\"\n",
    "        # Convert string-type date to datetime object.\n",
    "        if type(date) is str:\n",
    "            date = pd.to_datetime(date)\n",
    "        \n",
    "        # Assign weekday if not given.\n",
    "        if weekday is None:\n",
    "            weekday = date.weekday()\n",
    "        \n",
    "        # Assign holiday category if not given.\n",
    "        if holiday is None:\n",
    "            import holidays\n",
    "            holiday = date in holidays.country_holidays(country_code)\n",
    "        \n",
    "        # Special treatment for Christmas eve and New year's eve as Saturdays.\n",
    "        if ( date.day == 24 or date.day == 31 ) and date.month == 12 and weekday != 6:\n",
    "            pslp_category = 1\n",
    "        # weekdays\n",
    "        elif weekday < 5 and holiday is False:\n",
    "            pslp_category = 0\n",
    "        # Saturdays\n",
    "        elif weekday == 5 and holiday is False:\n",
    "            pslp_category = 1\n",
    "        # Sundays and holidays\n",
    "        elif weekday == 6 or holiday is True:\n",
    "            pslp_category = 2\n",
    "        return pslp_category\n",
    "    \n",
    "    \n",
    "    def _assign_pslp_categories(self, country_code=\"DE\"):\n",
    "        \"\"\"\n",
    "        Assign PSLP categories to dates in dataframe's datetime index.\n",
    "    \n",
    "        0 is weekday, 1 is Saturday, 2 is Sunday or holiday.\n",
    "        Special treatment for Christmas eve and New Year's eve (as Saturdays).\n",
    "    \n",
    "        Params\n",
    "        ------\n",
    "        df : pandas.Dataframe\n",
    "        country_code : str\n",
    "                       country to determine holidays for\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Dataframe\n",
    "        Dataframe amended by weekday information, holiday information, and PSLP category\n",
    "        \"\"\"\n",
    "    \n",
    "        import holidays\n",
    "        \n",
    "        # Get holidays in specified country.\n",
    "        country_holidays = holidays.country_holidays(country_code) # Passing a state is also possible!\n",
    "    \n",
    "        s = self.df.index.to_series()                                # Convert datetime index to series.\n",
    "        dates = s.dt.date                                       # Get plain dates from datetime objects.\n",
    "        weekdays = s.dt.weekday                                 # Get weekdays from datetime objects.\n",
    "        holidays = [date in country_holidays for date in dates] # Determine holidays.\n",
    "        pslp_category = []\n",
    "        \n",
    "        for d, wd, hd in zip(dates, weekdays, holidays):\n",
    "            pslp_category.append(self.get_pslp_category(d, wd, hd))\n",
    "            \n",
    "        self.df[\"PSLP Category\"] = pslp_category\n",
    "        self.df[\"Holiday\"] = holidays\n",
    "        self.df[\"Weekday\"] = weekdays\n",
    "    \n",
    "    \n",
    "    def calculate_residuals(self):\n",
    "        \"\"\"\n",
    "        Calculate residuals of actual data w.r.t PSLPs.\n",
    "        \"\"\"\n",
    "        for header in self.original_headers:\n",
    "            self.df[header+\" Residuals\"] = self.df[header] - self.df[header+\" SARIMA\"]\n",
    "    \n",
    "    \n",
    "    def plot_data(self):\n",
    "        \"\"\"\n",
    "        Plot preprocessed load and generation data.\n",
    "        \"\"\"\n",
    "        from plotly.subplots import make_subplots\n",
    "        import plotly.graph_objects as go\n",
    "    \n",
    "        num_rows = len(self.original_headers)\n",
    "        \n",
    "        fig = make_subplots(rows=num_rows, cols=1, subplot_titles=(self.original_headers))\n",
    "    \n",
    "        for i, header in enumerate(self.original_headers):\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header], name=header), row=i+1, col=1)\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header+\" PSLP\"], name=header+\" PSLP\"), row=i+1, col=1)\n",
    "            fig.add_trace(go.Scatter(x = self.df.index, y = self.df[header+\" Residuals\"], name=header+\" Residuals\"), row=i+1, col=1)\n",
    "    \n",
    "        fig.update_layout(height=10000, width=1200)\n",
    "        fig.show()\n",
    "        \n",
    "       \n",
    "    def calculate_errors(self):\n",
    "        \"\"\"\n",
    "        Calculate forecasting errors for preprocessed ENTSO-E load and generation data.  \n",
    "        \"\"\"\n",
    "        from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "        errors = {}\n",
    "        for header in self.original_headers:\n",
    "            temp = pd.concat([self.df[header], self.df[header+\" PSLP\"]], axis=1).dropna()\n",
    "            mae = mean_absolute_error(temp[header], temp[header+\" PSLP\"])\n",
    "            mape = mean_absolute_percentage_error(temp[header], temp[header+\" PSLP\"])\n",
    "            mse = mean_squared_error(temp[header], temp[header+\" PSLP\"])\n",
    "            errors[header] = {\"MAE\": mae, \"MAPE\": mape, \"MSE\": mse}\n",
    "        self.errors = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251161f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SarimaData = SARIMADataset(start_date, end_date, api_key, country_code, time_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(SarimaData.df[\"Actual Load\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f59c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.75\n",
    "seasonal_cycle = 96\n",
    "overall_days = int(SarimaData.df.shape[0] / seasonal_cycle)\n",
    "train_days = int(train_fraction * overall_days)\n",
    "test_days = overall_days - train_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO!\n",
    "def plot_autocorrelations(data):\n",
    "    plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n",
    "    # Original Series\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    axes[0, 0].plot(data_train[dates_train[0]].values); axes[0, 0].set_title(f'Original Series {header}')\n",
    "    plot_acf(data_train[dates_train[0]].values, ax=axes[0, 1])\n",
    "    plot_pacf(data_train[dates_train[0]].values, ax=axes[0, 1])\n",
    "\n",
    "    # 1st Differencing\n",
    "    axes[1, 0].plot(data_train[dates_train[0]].diff().values); axes[1, 0].set_title(f'1st Order Differencing {header}')\n",
    "    plot_acf(data_train[dates_train[0]].diff().dropna().values, ax=axes[1, 1])\n",
    "    plot_acf(data_train[dates_train[0]].values, ax=axes[0, 1])\n",
    "\n",
    "    \n",
    "    # 2nd Differencing\n",
    "    axes[2, 0].plot(data_train[dates_train[0]].diff().diff().values); axes[2, 0].set_title(f'2nd Order Differencing {header}')\n",
    "    plot_acf(data_train[dates_train[0]].diff().diff().dropna().values, ax=axes[2, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0dfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SarimaData.df[\"Actual Load\"]\n",
    "result = auto_arima(data.values, m=seasonal_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in SarimaData.original_headers:\n",
    "    # Original data.\n",
    "    data = SarimaData.df[header]\n",
    "    dates = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "    # Get train fraction from original data.\n",
    "    data_train = SarimaData.df[header].head(train_days * seasonal_cycle)\n",
    "    dates_train = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data_train.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "    # Get test fraction from original data.\n",
    "    data_test = SarimaData.df[header].tail(test_days * seasonal_cycle)\n",
    "    dates_test = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in data_test.index.to_series().dt.date.drop_duplicates().tolist()]\n",
    "\n",
    "    # pmdarima.arima.auto_arima(y, m=1, seasonal=True, stationary=False, information_criterion='aic', alpha=0.05, test='kpss', seasonal_test='ocsb', trend=None, method='lbfgs')\n",
    "    \n",
    "    auto_arima(data.values, m=seasonal_cycle, seasonal=True, stationary=False, information_criterion='aic', test='kpss', seasonal_test='ocsb', trend=None, method='lbfgs')\n",
    "    \n",
    "    \n",
    "    # p = ndiffs(data.values, test='adf')\n",
    "    \n",
    "    # Estimate order of differencing, d.\n",
    "    # Perform a stationarity test for different levels of d to estimate the number of differences \n",
    "    # required to make a given time series stationary. Will select the maximum value of d for \n",
    "    # which the time series is judged stationary by the statistical test.\n",
    "    # Default type of unit root test of stationarity: KPSS\n",
    "    # Kwiatkowski–Phillips–Schmidt–Shin (KPSS) tests are used for testing a null hypothesis \n",
    "    # that an observable time series is stationary around a deterministic trend \n",
    "    # (i.e. trend-stationary) against the alternative of a unit root.\n",
    "    # d = ndiffs(data.values, test='kpss')\n",
    "    \n",
    "    # pmdarima.arima.nsdiffs(x, m, max_D=2, test='ocsb')\n",
    "    # Estimate the seasonal differencing term, D.\n",
    "    # Perform a test of seasonality for different levels of D to estimate the number of \n",
    "    # seasonal differences required to make a given time series stationary. \n",
    "    # Will select the maximum value of D for which the time series is judged \n",
    "    # seasonally stationary by the statistical test.\n",
    "    # Default type of unit root test of stationarity: OCSB\n",
    "    # D = nsdiffs(data.values, seasonal_cycle, test='ocsb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150eaa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
